{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import get_param_num\n",
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention_block import Attention_Block\n",
    "Attention_Block()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0430, 0.9147, 0.1368, 0.5885, 0.4144, 0.7169, 0.0027, 0.0746, 0.4161,\n",
      "        0.3880, 0.3759, 0.0615, 0.0412, 0.3498, 0.6788, 0.6110, 0.5153, 0.5850,\n",
      "        0.6866, 0.8740, 0.5532, 0.4790, 0.9969, 0.0618, 0.9934, 0.9529, 0.1372,\n",
      "        0.6175, 0.7070, 0.6185, 0.8559, 0.2855, 0.6112, 0.2164, 0.9712, 0.7035,\n",
      "        0.9965, 0.7054, 0.0127, 0.6756, 0.6323, 0.0121, 0.8615, 0.3093, 0.0826,\n",
      "        0.1075, 0.2682, 0.4951, 0.8457, 0.9204, 0.0221, 0.6619, 0.0107, 0.0739,\n",
      "        0.2116, 0.4047, 0.7988, 0.1389, 0.5971, 0.8722, 0.5600, 0.4953, 0.4316,\n",
      "        0.4316, 0.2752, 0.6699, 0.3131, 0.4004, 0.7912, 0.0869, 0.4410, 0.4506,\n",
      "        0.0575, 0.1379, 0.2720, 0.0864, 0.3694, 0.4746, 0.8273, 0.6181, 0.7925,\n",
      "        0.8974, 0.3565, 0.7967, 0.2007, 0.6868, 0.7469, 0.2755, 0.1016, 0.7986,\n",
      "        0.9311, 0.0448, 0.8961, 0.5783, 0.0910, 0.2596, 0.4325, 0.7601, 0.6582,\n",
      "        0.6196, 0.6497, 0.0870, 0.5796, 0.0573, 0.8086, 0.0533, 0.9792, 0.5432,\n",
      "        0.1177, 0.4214, 0.1680, 0.0209, 0.9951, 0.8920, 0.7532, 0.7037, 0.0146,\n",
      "        0.0945, 0.3966, 0.1739, 0.4844, 0.3500, 0.0766])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0430, 0.9147, 0.1368, 0.5885, 0.4144, 0.7169, 0.0027, 0.0746, 0.4161,\n",
       "        0.3880, 0.3759, 0.0615, 0.0412, 0.3498, 0.6788, 0.6110, 0.5153, 0.5850,\n",
       "        0.6866, 0.8740, 0.5532, 0.4790, 0.9969, 0.0618, 0.9934, 0.9529, 0.1372,\n",
       "        0.6175, 0.7070, 0.6185, 0.8559, 0.2855, 0.6112, 0.2164, 0.9712, 0.7035,\n",
       "        0.9965, 0.7054, 0.0127, 0.6756, 0.6323, 0.0121, 0.8615, 0.3093, 0.0826,\n",
       "        0.1075, 0.2682, 0.4951, 0.8457, 0.9204, 0.0221, 0.6619, 0.0107, 0.0739,\n",
       "        0.2116, 0.4047, 0.7988, 0.1389, 0.5971, 0.8722, 0.5600, 0.4953, 0.4316,\n",
       "        0.4316, 0.2752, 0.6699, 0.3131, 0.4004, 0.7912, 0.0869, 0.4410, 0.4506,\n",
       "        0.0575, 0.1379, 0.2720, 0.0864, 0.3694, 0.4746, 0.8273, 0.6181, 0.7925,\n",
       "        0.8974, 0.3565, 0.7967, 0.2007, 0.6868, 0.7469, 0.2755, 0.1016, 0.7986,\n",
       "        0.9311, 0.0448, 0.8961, 0.5783, 0.0910, 0.2596, 0.4325, 0.7601, 0.6582,\n",
       "        0.6196, 0.6497, 0.0870, 0.5796, 0.0573, 0.8086, 0.0533, 0.9792, 0.5432,\n",
       "        0.1177, 0.4214, 0.1680, 0.0209, 0.9951, 0.8920, 0.7532, 0.7037, 0.0146,\n",
       "        0.0945, 0.3966, 0.1739, 0.4844, 0.3500, 0.0766],\n",
       "       grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b= nn.Linear(123,123,bias = False\n",
    "             )\n",
    "b.weight.data = torch.eye(123)\n",
    "\n",
    "print(a)\n",
    "b(a) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 49472\n"
     ]
    }
   ],
   "source": [
    "from feedForward import Feed_Forward\n",
    "\n",
    "\n",
    "input_size = 128\n",
    "output_size = 64\n",
    "model = Feed_Forward(input_size, output_size)\n",
    "\n",
    "# Count the number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2607347938.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_8624\\2607347938.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    (1,128) X (128,256) => (1,256) X (1,64)\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "(1,128) X (128,256) => (1,256) X (1,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0875, 0.0803, 0.1061, 0.0000, 0.0000, 0.0973,\n",
       "         0.0000, 0.0476, 0.0000, 0.2296, 0.1238, 0.0000, 0.0000, 0.1309, 0.0000,\n",
       "         0.0204, 0.1597, 0.0000, 0.1159, 0.0679, 0.0000, 0.2465, 0.0366, 0.0000,\n",
       "         0.0000, 0.0277, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0954, 0.0000, 0.0178, 0.0000, 0.1735, 0.0997, 0.0142, 0.1003, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0336, 0.1102, 0.1245, 0.0000, 0.0000, 0.1014,\n",
       "         0.0257, 0.0000, 0.0000, 0.1484, 0.0000, 0.0000, 0.0000, 0.0138, 0.0000,\n",
       "         0.2103]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.rand((1,128))\n",
    "model(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 141952\n"
     ]
    }
   ],
   "source": [
    "from encoder import Encoder\n",
    "enc = Encoder(6,16,10,32,32,32)\n",
    "\n",
    "total_params = sum(p.numel() for p in enc.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "test = torch.rand((5,16))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot assign 'torch.FloatTensor' as child module 'Q' (torch.nn.Module or None expected)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18656\\2663831095.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0matt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\projects\\TORCH\\attention_block.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, embedding)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m#linear transformation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   1218\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mmodules\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1220\u001b[1;33m                     raise TypeError(\"cannot assign '{}' as child module '{}' \"\n\u001b[0m\u001b[0;32m   1221\u001b[0m                                     \u001b[1;34m\"(torch.nn.Module or None expected)\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1222\u001b[0m                                     .format(torch.typename(value), name))\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot assign 'torch.FloatTensor' as child module 'Q' (torch.nn.Module or None expected)"
     ]
    }
   ],
   "source": [
    "att(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention_Block(\n",
      "  (Q): Linear(in_features=16, out_features=320, bias=False)\n",
      "  (V): Linear(in_features=16, out_features=320, bias=False)\n",
      "  (K): Linear(in_features=16, out_features=320, bias=False)\n",
      "  (conc_heads): Linear(in_features=320, out_features=16, bias=True)\n",
      ") Feed_Forward(\n",
      "  (l1): Linear(in_features=32, out_features=64, bias=True)\n",
      "  (l2): Linear(in_features=64, out_features=16, bias=True)\n",
      ")\n",
      "42342424\n",
      "Attention_Block(\n",
      "  (Q): Linear(in_features=16, out_features=320, bias=False)\n",
      "  (V): Linear(in_features=16, out_features=320, bias=False)\n",
      "  (K): Linear(in_features=16, out_features=320, bias=False)\n",
      "  (conc_heads): Linear(in_features=320, out_features=16, bias=True)\n",
      ") Feed_Forward(\n",
      "  (l1): Linear(in_features=32, out_features=64, bias=True)\n",
      "  (l2): Linear(in_features=64, out_features=16, bias=True)\n",
      ")\n",
      "42342424\n",
      "Attention_Block(\n",
      "  (Q): Linear(in_features=16, out_features=320, bias=False)\n",
      "  (V): Linear(in_features=16, out_features=320, bias=False)\n",
      "  (K): Linear(in_features=16, out_features=320, bias=False)\n",
      "  (conc_heads): Linear(in_features=320, out_features=16, bias=True)\n",
      ") Feed_Forward(\n",
      "  (l1): Linear(in_features=32, out_features=64, bias=True)\n",
      "  (l2): Linear(in_features=64, out_features=16, bias=True)\n",
      ")\n",
      "42342424\n",
      "Attention_Block(\n",
      "  (Q): Linear(in_features=16, out_features=320, bias=False)\n",
      "  (V): Linear(in_features=16, out_features=320, bias=False)\n",
      "  (K): Linear(in_features=16, out_features=320, bias=False)\n",
      "  (conc_heads): Linear(in_features=320, out_features=16, bias=True)\n",
      ") Feed_Forward(\n",
      "  (l1): Linear(in_features=32, out_features=64, bias=True)\n",
      "  (l2): Linear(in_features=64, out_features=16, bias=True)\n",
      ")\n",
      "42342424\n",
      "Attention_Block(\n",
      "  (Q): Linear(in_features=16, out_features=320, bias=False)\n",
      "  (V): Linear(in_features=16, out_features=320, bias=False)\n",
      "  (K): Linear(in_features=16, out_features=320, bias=False)\n",
      "  (conc_heads): Linear(in_features=320, out_features=16, bias=True)\n",
      ") Feed_Forward(\n",
      "  (l1): Linear(in_features=32, out_features=64, bias=True)\n",
      "  (l2): Linear(in_features=64, out_features=16, bias=True)\n",
      ")\n",
      "42342424\n",
      "Attention_Block(\n",
      "  (Q): Linear(in_features=16, out_features=320, bias=False)\n",
      "  (V): Linear(in_features=16, out_features=320, bias=False)\n",
      "  (K): Linear(in_features=16, out_features=320, bias=False)\n",
      "  (conc_heads): Linear(in_features=320, out_features=16, bias=True)\n",
      ") Feed_Forward(\n",
      "  (l1): Linear(in_features=32, out_features=64, bias=True)\n",
      "  (l2): Linear(in_features=64, out_features=16, bias=True)\n",
      ")\n",
      "42342424\n"
     ]
    }
   ],
   "source": [
    "for  att,ff in enc.blocks:\n",
    "    print(att,ff)\n",
    "    print(42342424)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3674, 0.7387, 0.1723, 0.2291, 0.0548, 0.4521, 0.2220, 0.8759, 0.9160,\n",
       "         0.0775, 0.8661, 0.8004, 0.0339, 0.4578, 0.2771, 0.9520],\n",
       "        [0.7591, 0.7433, 0.3570, 0.3890, 0.8373, 0.8834, 0.9863, 0.5260, 0.7355,\n",
       "         0.7413, 0.4700, 0.0075, 0.4724, 0.4950, 0.2829, 0.6522],\n",
       "        [0.3442, 0.9564, 0.9601, 0.3779, 0.2610, 0.5725, 0.0811, 0.6971, 0.7743,\n",
       "         0.7053, 0.4369, 0.1443, 0.1607, 0.3055, 0.4420, 0.7519],\n",
       "        [0.4820, 0.7941, 0.3467, 0.1407, 0.0290, 0.2657, 0.2115, 0.8567, 0.1753,\n",
       "         0.4351, 0.9540, 0.6656, 0.5675, 0.8200, 0.8916, 0.9789],\n",
       "        [0.8254, 0.4508, 0.3901, 0.9171, 0.5026, 0.3376, 0.9864, 0.1767, 0.6384,\n",
       "         0.4813, 0.1636, 0.5627, 0.7647, 0.2236, 0.2098, 0.9506]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Encoder(\n",
       "  (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       ")>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Encoder(\n",
       "  (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       ")>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
